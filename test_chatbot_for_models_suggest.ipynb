{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd4hLGLy3UPa",
        "outputId": "4c87b2fb-960d-4709-e695-94b879d43a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.6.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.28.2 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.2 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 grpcio-1.67.0 grpcio-tools-1.67.0 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.6 httpx-0.27.2 hyperframe-6.0.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.0 langchain-huggingface-0.1.0 langchain-text-splitters-0.3.0 langchain_community-0.3.0 langchain_core-0.3.5 langsmith-0.1.135 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 pandas-2.2.3 portalocker-2.10.1 protobuf-5.28.2 pydantic-settings-2.5.2 python-dotenv-1.0.1 qdrant-client-1.11.2 requests-toolbelt-1.0.0 sentence_transformers-3.1.1 tenacity-8.5.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ngocdai101004/chatbot-for-model-suggestion.git\n",
        "%cd chatbot-for-model-suggestion\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python down_save_models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1faA2UE4PYq",
        "outputId": "b233a126-f21a-4abf-e0f8-e57f65dc8482"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json: 100% 687/687 [00:00<00:00, 5.17MB/s]\n",
            "pytorch_model.bin: 100% 2.27G/2.27G [00:04<00:00, 468MB/s]\n",
            "tokenizer_config.json: 100% 444/444 [00:00<00:00, 3.15MB/s]\n",
            "sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 273MB/s]\n",
            "tokenizer.json: 100% 17.1M/17.1M [00:00<00:00, 335MB/s]\n",
            "special_tokens_map.json: 100% 964/964 [00:00<00:00, 5.88MB/s]\n",
            "config.json: 100% 799/799 [00:00<00:00, 5.42MB/s]\n",
            "model.safetensors: 100% 1.11G/1.11G [00:02<00:00, 475MB/s]\n",
            "tokenizer_config.json: 100% 443/443 [00:00<00:00, 3.53MB/s]\n",
            "sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 328MB/s]\n",
            "tokenizer.json: 100% 17.1M/17.1M [00:00<00:00, 401MB/s]\n",
            "special_tokens_map.json: 100% 279/279 [00:00<00:00, 1.79MB/s]\n",
            "config.json: 100% 1.15k/1.15k [00:00<00:00, 9.31MB/s]\n",
            "model.safetensors: 100% 1.63G/1.63G [00:03<00:00, 448MB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 206kB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 4.18MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 18.9MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 6.05MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
            "config.json: 100% 683/683 [00:00<00:00, 5.21MB/s]\n",
            "model.safetensors.index.json: 100% 35.6k/35.6k [00:00<00:00, 134MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/3.97G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/3.97G [00:00<01:25, 46.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 73.4M/3.97G [00:00<00:14, 263MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 136M/3.97G [00:00<00:10, 363MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 189M/3.97G [00:00<00:09, 409MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 241M/3.97G [00:00<00:08, 421MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 304M/3.97G [00:00<00:07, 462MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 367M/3.97G [00:00<00:07, 496MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 430M/3.97G [00:01<00:06, 523MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 493M/3.97G [00:01<00:06, 542MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 556M/3.97G [00:01<00:06, 548MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 619M/3.97G [00:01<00:06, 551MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 682M/3.97G [00:01<00:05, 553MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 744M/3.97G [00:01<00:05, 557MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 807M/3.97G [00:01<00:05, 557MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 870M/3.97G [00:01<00:05, 551MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 933M/3.97G [00:01<00:05, 540MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 996M/3.97G [00:02<00:05, 543MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.06G/3.97G [00:02<00:05, 543MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.12G/3.97G [00:02<00:05, 535MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.18G/3.97G [00:02<00:05, 522MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.24G/3.97G [00:02<00:05, 498MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.29G/3.97G [00:02<00:05, 485MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.34G/3.97G [00:02<00:05, 489MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.39G/3.97G [00:02<00:05, 477MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.45G/3.97G [00:02<00:05, 473MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.50G/3.97G [00:03<00:05, 476MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.55G/3.97G [00:03<00:05, 444MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.60G/3.97G [00:03<00:05, 434MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 1.66G/3.97G [00:03<00:05, 436MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 1.71G/3.97G [00:03<00:05, 438MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 1.76G/3.97G [00:03<00:04, 453MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 1.81G/3.97G [00:03<00:04, 455MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 1.87G/3.97G [00:03<00:04, 454MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 1.92G/3.97G [00:04<00:04, 463MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 1.97G/3.97G [00:04<00:04, 463MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.02G/3.97G [00:04<00:04, 394MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.08G/3.97G [00:04<00:04, 410MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.13G/3.97G [00:04<00:04, 394MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.17G/3.97G [00:04<00:04, 394MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.21G/3.97G [00:04<00:04, 372MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.26G/3.97G [00:04<00:04, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.32G/3.97G [00:05<00:04, 384MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.37G/3.97G [00:05<00:03, 417MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 2.42G/3.97G [00:05<00:04, 376MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 2.46G/3.97G [00:05<00:04, 308MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 2.53G/3.97G [00:05<00:03, 361MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 2.59G/3.97G [00:05<00:03, 394MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 2.63G/3.97G [00:05<00:03, 395MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 2.68G/3.97G [00:06<00:03, 415MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 2.74G/3.97G [00:06<00:04, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 2.80G/3.97G [00:06<00:03, 309MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 2.85G/3.97G [00:06<00:03, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 2.92G/3.97G [00:06<00:02, 392MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 2.98G/3.97G [00:06<00:02, 425MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.04G/3.97G [00:06<00:02, 461MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.10G/3.97G [00:07<00:01, 488MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.16G/3.97G [00:07<00:02, 354MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 3.21G/3.97G [00:07<00:02, 380MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 3.26G/3.97G [00:07<00:01, 355MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 3.31G/3.97G [00:07<00:01, 390MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 3.37G/3.97G [00:07<00:01, 411MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 3.43G/3.97G [00:07<00:01, 420MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 3.48G/3.97G [00:08<00:01, 399MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 3.52G/3.97G [00:08<00:01, 379MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 3.57G/3.97G [00:08<00:01, 339MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 3.62G/3.97G [00:08<00:00, 380MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 3.67G/3.97G [00:08<00:00, 381MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 3.71G/3.97G [00:08<00:00, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 3.76G/3.97G [00:08<00:00, 373MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 3.83G/3.97G [00:09<00:00, 418MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 3.88G/3.97G [00:09<00:00, 444MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 3.97G/3.97G [00:09<00:00, 423MB/s]\n",
            "Downloading shards:  50% 1/2 [00:09<00:09,  9.63s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.20G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 52.4M/2.20G [00:00<00:04, 496MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 115M/2.20G [00:00<00:03, 527MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 178M/2.20G [00:00<00:03, 520MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 241M/2.20G [00:00<00:03, 528MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 304M/2.20G [00:00<00:03, 527MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 367M/2.20G [00:00<00:05, 326MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 430M/2.20G [00:01<00:04, 373MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 493M/2.20G [00:01<00:04, 415MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 556M/2.20G [00:01<00:03, 444MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 608M/2.20G [00:01<00:03, 455MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 661M/2.20G [00:01<00:03, 463MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 713M/2.20G [00:01<00:04, 316MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 776M/2.20G [00:01<00:03, 369MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 839M/2.20G [00:02<00:03, 415MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 891M/2.20G [00:02<00:03, 427MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 944M/2.20G [00:02<00:02, 432MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 996M/2.20G [00:02<00:03, 365MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.04G/2.20G [00:02<00:03, 327MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.08G/2.20G [00:02<00:03, 298MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.12G/2.20G [00:02<00:03, 280MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.15G/2.20G [00:03<00:03, 264MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.18G/2.20G [00:03<00:04, 252MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.22G/2.20G [00:03<00:04, 239MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.25G/2.20G [00:03<00:04, 227MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.28G/2.20G [00:03<00:04, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.31G/2.20G [00:03<00:04, 203MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.33G/2.20G [00:04<00:04, 197MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 1.35G/2.20G [00:04<00:04, 189MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.37G/2.20G [00:04<00:04, 183MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.43G/2.20G [00:04<00:02, 263MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.48G/2.20G [00:04<00:02, 326MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.54G/2.20G [00:04<00:01, 390MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.59G/2.20G [00:04<00:01, 423MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 1.65G/2.20G [00:04<00:01, 434MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 1.70G/2.20G [00:04<00:01, 444MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 1.75G/2.20G [00:05<00:00, 453MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 1.80G/2.20G [00:05<00:00, 455MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 1.86G/2.20G [00:05<00:00, 454MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 1.91G/2.20G [00:05<00:00, 455MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 1.96G/2.20G [00:05<00:00, 463MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.01G/2.20G [00:05<00:00, 474MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.07G/2.20G [00:05<00:00, 479MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.12G/2.20G [00:05<00:00, 461MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.20G/2.20G [00:06<00:00, 367MB/s]\n",
            "Downloading shards: 100% 2/2 [00:15<00:00,  7.94s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.16it/s]\n",
            "generation_config.json: 100% 138/138 [00:00<00:00, 1.02MB/s]\n",
            "tokenizer_config.json: 100% 7.23k/7.23k [00:00<00:00, 36.1MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 32.8MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:01<00:00, 1.57MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 24.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtvXx0Sq6Nwq",
        "outputId": "64850642-59aa-4502-e3a3-a5795bb60fc9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_O1_Sxt6fNh",
        "outputId": "8c6ff271-c0e2-44c9-e226-36083f8dee5f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "2024-10-16 18:47:05.347483: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-16 18:47:05.366061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-16 18:47:05.387921: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-16 18:47:05.394549: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-16 18:47:05.410744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-16 18:47:06.641663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/chatbot-for-model-suggestion/chatbot-for-model-suggestion/chatbot-for-model-suggestion/src/ingest/retriever.py:20: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  self.embedding = HuggingFaceEmbeddings(\n",
            "No sentence-transformers model found with name src/models/BAAI_bge-m3. Creating a new one with mean pooling.\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.50s/it]\n",
            "Enter your query: Give me some model about diffusion topic\n",
            "Score:  0.9497306942939758\n",
            "------------------Response-------------------\n",
            " \n",
            "For text-to-image generation, here are a few models that could be of interest:\n",
            "\n",
            "1. **Stable Diffusion v1.4 (Rating: 3.8 star, Score: 71)**: This model is known for its ability to generate high-quality, photorealistic images from text descriptions, which makes it suitable for creative projects, art generation, and various digital content applications. It has been downloaded 2 times and is available at [https://www.inferium.io/#/inferenced-models/863864469/model-card](https://www.inferium.io/#/inferenced-models/863864469/model-card).\n",
            "\n",
            "2. **Stable Diffusion 3 Medium Diffusers (Rating: 4.8 star, Score: 71)**: This medium-sized diffusion model is developed by Stability AI and offers high-quality, detailed image generation from text prompts. It is designed for creative and artistic applications. It has been downloaded 5 times and is available at [https://www.inferium.io/#/inferenced-models/838809514/model-card](https://www.inferium.io/#/inferenced-models/838809514/model-card).\n",
            "\n",
            "3. **Stable Diffusion v1.5 (Rating: 3.5 star, Score: 72)**: This advanced text-to-image model improves on previous versions with enhanced realism and versatility. It supports a wide range of creative applications, including digital art, design, and content creation. It has been downloaded 0 times and is available at [https://www.inferium.io/#/inferenced-models/863871525/model-card](https://www.inferium.io/#/inferenced-models/863871525/model-card).\n",
            "\n",
            "4. **Stable Diffusion XL Base 1.0 (Rating: 4.4 star, Score: 91)**: This cutting-edge text-to-image model delivers high-quality, detailed image generation from text prompts. It supports various creative applications such as digital art, design, and content creation. It has not been downloaded yet and is available at [https://www.inferium.io/#/inferenced-models/864596746/model-card](https://www.inferium.io/#/inferenced-models/8645\n",
            "------------------History------------------- \n",
            " 2\n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xOoM78MInR6y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}